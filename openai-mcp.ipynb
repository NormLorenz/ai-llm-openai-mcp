{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvArPLaucLgSVayXn4HqwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NormLorenz/ai-llm-openai-mcp/blob/main/openai-mcp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OpenAI with a MCP Server"
      ],
      "metadata": {
        "id": "jHp8SlW7Qhtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install fastmcp openai nest_asyncio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g1sQZk15Rozz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell sets up and starts a FastMCP server. It defines several functionalities: Tools like get_forecast, get_alerts, multiply, and health_check which perform specific actions. It also registers Resources such as climate_data, get_greeting, and get_config to provide data. Additionally, it includes a Prompt called analyze_data for generating structured prompts. The server is then launched in a background thread to make these tools and resources accessible via HTTP, specifically configured for notebook compatibility."
      ],
      "metadata": {
        "id": "rcxTOVG9H5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The MCP Server\n",
        "\n",
        "from fastmcp import FastMCP\n",
        "import nest_asyncio\n",
        "import threading\n",
        "import time\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "mcp = FastMCP(\n",
        "    name=\"WeatherServer\",\n",
        "    instructions=\"This provides an up to date weather forecast for any location.\"\n",
        ")\n",
        "\n",
        "# Tool 1: Forecast\n",
        "@mcp.tool(\"get_forecast\")\n",
        "def get_forecast(location: str):\n",
        "    \"\"\"Returns a forecast for the given location.\"\"\"\n",
        "    return {\"forecast\": f\"Sunny in {location}\"}\n",
        "\n",
        "# Tool 2: Alerts\n",
        "@mcp.tool(name=\"get_alerts\")\n",
        "def get_alerts(location: str):\n",
        "    \"\"\"Returns any severe weather alerts for the given location.\"\"\"\n",
        "    return {\"alerts\": f\"No severe alerts currently for {location}\"}\n",
        "\n",
        "# Tool 3: Math\n",
        "@mcp.tool\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplies two numbers together.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# Tool 4: Health Check\n",
        "@mcp.tool\n",
        "def health_check():\n",
        "    \"\"\"Returns the health status of the server.\"\"\"\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "# Resource 1: Climate Data\n",
        "@mcp.resource(uri=\"resource://climate\")\n",
        "def climate_data():\n",
        "    \"\"\"Return static climate information.\"\"\"\n",
        "    return {\n",
        "        \"Berlin\": {\"avg_temp\": \"10¬∞C\", \"rainfall\": \"570mm\"},\n",
        "        \"Boise\": {\"avg_temp\": \"12¬∞C\", \"rainfall\": \"300mm\"},\n",
        "        \"Tokyo\": {\"avg_temp\": \"16¬∞C\", \"rainfall\": \"1500mm\"}\n",
        "    }\n",
        "\n",
        "# Resource 2: Basic dynamic resource returning a string\n",
        "@mcp.resource(\"resource://greeting\")\n",
        "def get_greeting() -> str:\n",
        "    \"\"\"Provides a simple greeting message.\"\"\"\n",
        "    return \"Hello from FastMCP Resources!\"\n",
        "\n",
        "# Resource 3: Resource returning JSON data (dict is auto-serialized)\n",
        "@mcp.resource(\"data://config\")\n",
        "def get_config() -> dict:\n",
        "    \"\"\"Provides application configuration as JSON.\"\"\"\n",
        "    return {\n",
        "        \"theme\": \"dark\",\n",
        "        \"version\": \"1.2.0\",\n",
        "        \"features\": [\"tools\", \"resources\"],\n",
        "    }\n",
        "\n",
        "# Resource 4: Resource returning secret data\n",
        "@mcp.resource(\"data://secret\", enabled=False)\n",
        "def get_secret_data():\n",
        "    \"\"\"This resource is currently disabled.\"\"\"\n",
        "    return \"Secret data\"\n",
        "\n",
        "# Prompt 1: Analysis of Numerical data\n",
        "@mcp.prompt\n",
        "def analyze_data(data_points: list[float]) -> str:\n",
        "    \"\"\"Creates a prompt asking for analysis of numerical data.\"\"\"\n",
        "    formatted_data = \", \".join(str(point) for point in data_points)\n",
        "    return f\"Please analyze these data points: {formatted_data}\"\n",
        "\n",
        "# Define the function to run the server\n",
        "def run_server():\n",
        "    # Use transport=\"streamable-http\" for compatibility with notebooks/Colab\n",
        "    print(\"üöÄ Starting FastMCP server in background thread...\")\n",
        "    mcp.run(transport=\"streamable-http\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start the server in a separate thread\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Give the server a moment to start up\n",
        "time.sleep(5)\n",
        "print(\"‚úÖ Server should be running. Access it at http://localhost:8000/mcp\")"
      ],
      "metadata": {
        "id": "fYO0LykYQ05Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below sets up a test suite for the FastMCP server you started in the previous cell. It uses the fastmcp.Client to connect to the server and test its various functionalities: calling tools like get_forecast and multiply, reading resources such as climate_data, and retrieving prompts like analyze_data. The code then prints whether each test passed or failed, along with a summary of all tests. The output indicates that the client wasn't connected, which is a common issue when using fastmcp.Client outside of its recommended async with context manager."
      ],
      "metadata": {
        "id": "AxLFo7BzTWY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a MCP server test suite\n",
        "\n",
        "import asyncio\n",
        "from fastmcp import Client\n",
        "\n",
        "# MCP_SERVER_URL is available from previous cells in the kernel state.\n",
        "MCP_SERVER_URL = \"http://localhost:8000/mcp\"\n",
        "\n",
        "async def run_mcp_client_tests(server_url: str):\n",
        "    print(f\"üöÄ Starting FastMCP client tests against: {server_url}\")\n",
        "    print(\"Ensure the MCP server is running in a separate thread/process.\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    async with Client(server_url) as client:\n",
        "        print(\"\\n--- Inspecting Client Object ---\")\n",
        "        print(f\"Client type: {type(client)}\")\n",
        "        print(f\"Client dir: {dir(client)}\")\n",
        "        print(\"--------------------------------\")\n",
        "\n",
        "        # --- Test Tools ---\n",
        "        print(\"\\n--- Running Tool Tests ---\")\n",
        "        tool_test_cases = [\n",
        "            {'name': 'get_forecast', 'arguments': {'location': 'London'}, 'expected_result': {\"forecast\": \"Sunny in London\"}},\n",
        "            {'name': 'get_alerts', 'arguments': {'location': 'London'}, 'expected_result': {\"alerts\": \"No severe alerts currently for London\"}},\n",
        "            {'name': 'multiply', 'arguments': {'a': 5, 'b': 3}, 'expected_result': 15.0},\n",
        "            {'name': 'health_check', 'arguments': {}, 'expected_result': {\"status\": \"ok\"}}\n",
        "        ]\n",
        "\n",
        "        for test_case in tool_test_cases:\n",
        "            name = test_case['name']\n",
        "            args = test_case['arguments']\n",
        "            expected = test_case['expected_result']\n",
        "            print(f\"Testing tool: {name} with args: {args}\")\n",
        "            try:\n",
        "                result = await client.call_tool(name, arguments=args)\n",
        "                passed = (result.data == expected)\n",
        "                all_results.append({\n",
        "                    \"type\": \"tool\",\n",
        "                    \"name\": name,\n",
        "                    \"arguments\": args,\n",
        "                    \"passed\": passed,\n",
        "                    \"result\": result,\n",
        "                    \"expected\": expected\n",
        "                })\n",
        "                print(f\"  {'‚úÖ PASSED' if passed else '‚ùå FAILED'}: Result={result}\")\n",
        "            except Exception as e:\n",
        "                all_results.append({\n",
        "                    \"type\": \"tool\",\n",
        "                    \"name\": name,\n",
        "                    \"arguments\": args,\n",
        "                    \"passed\": False,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "                print(f\"  ‚ùå FAILED: Error={e}\")\n",
        "\n",
        "        # --- Test Resources ---\n",
        "        print(\"\\n--- Running Resource Tests ---\")\n",
        "        resource_test_cases = [\n",
        "            {'name': 'climate', 'expected_result': {\n",
        "                \"Berlin\": {\"avg_temp\": \"10¬∞C\", \"rainfall\": \"570mm\"},\n",
        "                \"Boise\": {\"avg_temp\": \"12¬∞C\", \"rainfall\": \"300mm\"},\n",
        "                \"Tokyo\": {\"avg_temp\": \"16¬∞C\", \"rainfall\": \"1500mm\"}\n",
        "            }}\n",
        "        ]\n",
        "\n",
        "        for test_case in resource_test_cases:\n",
        "            name = test_case['name']\n",
        "            expected = test_case['expected_result']\n",
        "            print(f\"Testing resource: {name}\")\n",
        "            try:\n",
        "                result = await client.read_resource(name)\n",
        "                passed = (result == expected)\n",
        "                all_results.append({\n",
        "                    \"type\": \"resource\",\n",
        "                    \"name\": name,\n",
        "                    \"passed\": passed,\n",
        "                    \"result\": result,\n",
        "                    \"expected\": expected\n",
        "                })\n",
        "                print(f\"  {'‚úÖ PASSED' if passed else '‚ùå FAILED'}: Result={result}\")\n",
        "            except Exception as e:\n",
        "                all_results.append({\n",
        "                    \"type\": \"resource\",\n",
        "                    \"name\": name,\n",
        "                    \"passed\": False,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "                print(f\"  ‚ùå FAILED: Error={e}\")\n",
        "\n",
        "        # --- Test Prompts ---\n",
        "        print(\"\\n--- Running Prompt Tests ---\")\n",
        "        prompt_test_cases = [\n",
        "            {'name': 'analyze_data', 'arguments': {'data_points': [10.5, 20.1, 30.0]}, 'expected_prefix': \"Please analyze these data points: 10.5, 20.1, 30.0\"}\n",
        "        ]\n",
        "\n",
        "        for test_case in prompt_test_cases:\n",
        "            name = test_case['name']\n",
        "            args = test_case['arguments']\n",
        "            expected_prefix = test_case['expected_prefix']\n",
        "            print(f\"Testing prompt: {name} with args: {args}\")\n",
        "            try:\n",
        "                result = await client.get_prompt(name, arguments=args)\n",
        "                # Correctly access the text content of the prompt message\n",
        "                message = result.messages[0]\n",
        "                passed = (message.content.text == expected_prefix)\n",
        "                all_results.append({\n",
        "                    \"type\": \"prompt\",\n",
        "                    \"name\": name,\n",
        "                    \"arguments\": args,\n",
        "                    \"passed\": passed,\n",
        "                    \"result\": result,\n",
        "                    \"expected\": expected_prefix\n",
        "                })\n",
        "                print(f\"  {'‚úÖ PASSED' if passed else '‚ùå FAILED'}: Result='{message.content.text}'\")\n",
        "            except Exception as e:\n",
        "                all_results.append({\n",
        "                    \"type\": \"prompt\",\n",
        "                    \"name\": name,\n",
        "                    \"arguments\": args,\n",
        "                    \"passed\": False,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "                print(f\"  ‚ùå FAILED: Error={e}\")\n",
        "\n",
        "    print(\"\\n--- Test Summary ---\")\n",
        "    total_tests = len(all_results)\n",
        "    passed_tests = sum(1 for r in all_results if r.get('passed', False))\n",
        "    failed_tests = total_tests - passed_tests\n",
        "\n",
        "    for res in all_results:\n",
        "        status = 'PASSED' if res.get('passed', False) else 'FAILED'\n",
        "        print(f\"[{res['type'].upper()}] {res['name']}: {status}\")\n",
        "        if not res.get('passed', False):\n",
        "            if 'error' in res:\n",
        "                print(f\"  Error: {res['error']}\")\n",
        "            else:\n",
        "                print(f\"  Expected: {res.get('expected')}\")\n",
        "                print(f\"  Got: {res.get('result')}\")\n",
        "    print(f\"\\nTotal Tests: {total_tests}, Passed: {passed_tests}, Failed: {failed_tests}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure to run the MCP server cell (fYO0LykYQ05Y) before running this test suite.\n",
        "    asyncio.run(run_mcp_client_tests(MCP_SERVER_URL))"
      ],
      "metadata": {
        "id": "puwEQK8jJfFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up an OpenAI agent to interact with your FastMCP server. It initializes an OpenAI client, defines the get_forecast tool in a format OpenAI understands, and then sends a user query about the weather to OpenAI. If OpenAI decides to use the get_forecast tool, the code extracts the tool's arguments, calls the corresponding tool on your FastMCP server, and then sends the tool's output back to OpenAI to generate a natural language response to the original query."
      ],
      "metadata": {
        "id": "So-5nyS0HtaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import asyncio\n",
        "from fastmcp import Client\n",
        "from google.colab import userdata\n",
        "\n",
        "# Print the key prefixes to help with any debugging\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if openai_api_key:\n",
        "    # print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "    print(f\"\\n--- OpenAI API Key exists and begins ---\\n{openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(f\"\\n--- OpenAI API Key not set ---\\n\")\n",
        "\n",
        "# Initialize the OPENAI_API_KEY as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Assuming MCP_SERVER_URL is defined from previous cells\n",
        "MCP_SERVER_URL = \"http://localhost:8000/mcp\"\n",
        "\n",
        "async def run_openai_agent():\n",
        "    # Initialize OpenAI client (ensure OPENAI_API_KEY is set as an environment variable)\n",
        "    client_openai = openai.OpenAI()\n",
        "\n",
        "    print(f\"\\n--- Connecting to MCP server at: ---\\n{MCP_SERVER_URL}\")\n",
        "\n",
        "    async with Client(MCP_SERVER_URL) as mcp_client:\n",
        "        # Define the tool for OpenAI in the required format\n",
        "        tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"get_forecast\",\n",
        "                    \"description\": \"Returns a forecast for the given location.\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"location\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"location\"]\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # User prompt\n",
        "        # user_prompt = \"What's the weather like in Paris?\"\n",
        "        user_prompt = \"What's the weather in Dresden because we want to eat lunch outside?\"\n",
        "        print(f\"\\n--- User Prompt ---\\n{user_prompt}\")\n",
        "\n",
        "        print(\"\\n--- OpenAI Agent Request ---\")\n",
        "        # Make a chat completion request to OpenAI, including the tool\n",
        "        response = client_openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\", # Or gpt-4, or other models that support tools\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            tools=tools,\n",
        "            tool_choice=\"auto\", # Let OpenAI decide if it wants to call the tool\n",
        "        )\n",
        "\n",
        "        response_message = response.choices[0].message\n",
        "\n",
        "        # Check if OpenAI decided to call a tool\n",
        "        if response_message.tool_calls:\n",
        "            tool_call = response_message.tool_calls[0]\n",
        "            function_name = tool_call.function.name\n",
        "            function_args = tool_call.function.arguments\n",
        "\n",
        "            print(f\"OpenAI wants to call tool: {function_name} with arguments: {function_args}\")\n",
        "\n",
        "            if function_name == \"get_forecast\":\n",
        "                # Parse the arguments string to a dictionary\n",
        "                import json\n",
        "                args_dict = json.loads(function_args)\n",
        "                location = args_dict.get(\"location\")\n",
        "\n",
        "                if location:\n",
        "                    print(f\"Calling MCP tool 'get_forecast' for location: {location}\")\n",
        "                    tool_response = await mcp_client.call_tool(function_name, arguments=args_dict)\n",
        "                    print(f\"MCP Tool Response: {tool_response.data}\")\n",
        "\n",
        "                    # Optionally, you can send the tool's output back to OpenAI for further processing\n",
        "                    second_response = client_openai.chat.completions.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"user\", \"content\": user_prompt},\n",
        "                            response_message, # the assistant's previous message with tool_calls\n",
        "                            {\n",
        "                                \"role\": \"tool\",\n",
        "                                \"tool_call_id\": tool_call.id,\n",
        "                                \"name\": function_name,\n",
        "                                \"content\": json.dumps(tool_response.data),\n",
        "                            },\n",
        "                        ],\n",
        "                    )\n",
        "                    print(\"\\n--- OpenAI Final Response ---\")\n",
        "                    print(second_response.choices[0].message.content)\n",
        "                else:\n",
        "                    print(\"Error: 'location' not found in tool arguments.\")\n",
        "            else:\n",
        "                print(f\"OpenAI suggested an unknown tool: {function_name}\")\n",
        "        else:\n",
        "            print(\"OpenAI did not suggest a tool call.\")\n",
        "            print(f\"OpenAI's response: {response_message.content}\")\n",
        "\n",
        "# Run the agent\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the MCP server cell (fYO0LykYQ05Y) has been executed\n",
        "    # and you have an OpenAI API key configured as an environment variable.\n",
        "    asyncio.run(run_openai_agent())\n"
      ],
      "metadata": {
        "id": "z6eub0YSKKnJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}