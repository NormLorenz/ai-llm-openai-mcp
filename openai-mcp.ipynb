{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNG/K1E/vzLDk/sa2KFPOEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NormLorenz/ai-llm-openai-mcp/blob/main/openai-mcp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OpenAI with a MCP Server"
      ],
      "metadata": {
        "id": "jHp8SlW7Qhtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip\n",
        "!pip install fastmcp openai nest_asyncio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g1sQZk15Rozz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your weather_server.py code\n",
        "from fastmcp import FastMCP\n",
        "import nest_asyncio\n",
        "import threading\n",
        "import time\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "mcp = FastMCP(\n",
        "    name=\"WeatherServerðŸš€\",\n",
        "    instructions=\"This provides an up to date weather forecast for any location.\"\n",
        ")\n",
        "\n",
        "# Tool 1: Forecast\n",
        "@mcp.tool(\"get_forecast\")\n",
        "def get_forecast(location: str):\n",
        "    return {\"forecast\": f\"Sunny in {location}\"}\n",
        "\n",
        "# Tool 2: Alerts\n",
        "@mcp.tool(name=\"get_alerts\")\n",
        "def get_alerts(location: str):\n",
        "    return {\"alerts\": f\"No severe alerts currently for {location}\"}\n",
        "\n",
        "# Tool 3: Math\n",
        "@mcp.tool\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplies two numbers together.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# Resource 1: Climate Data\n",
        "@mcp.resource(uri=\"http://example.com\", name=\"climate_data\")\n",
        "def climate_data():\n",
        "    \"\"\"Return static climate information.\"\"\"\n",
        "    return {\n",
        "        \"Berlin\": {\"avg_temp\": \"10Â°C\", \"rainfall\": \"570mm\"},\n",
        "        \"Boise\": {\"avg_temp\": \"12Â°C\", \"rainfall\": \"300mm\"},\n",
        "        \"Tokyo\": {\"avg_temp\": \"16Â°C\", \"rainfall\": \"1500mm\"}\n",
        "    }\n",
        "\n",
        "# Prompt 1: Analysis of Numerical data\n",
        "@mcp.prompt\n",
        "def analyze_data(data_points: list[float]) -> str:\n",
        "    \"\"\"Creates a prompt asking for analysis of numerical data.\"\"\"\n",
        "    formatted_data = \", \".join(str(point) for point in data_points)\n",
        "    return f\"Please analyze these data points: {formatted_data}\"\n",
        "\n",
        "# # Run server in background thread\n",
        "# def run_server():\n",
        "#     mcp.run(port=8000)\n",
        "\n",
        "# server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "# server_thread.start()\n",
        "\n",
        "# # Give the server time to start\n",
        "# time.sleep(3)\n",
        "# print(\"FastMCP server is running on port 8000\")\n",
        "\n",
        "# Define the function to run the server\n",
        "def run_server():\n",
        "  # Use transport=\"streamable-http\" for compatibility with notebooks/Colab\n",
        "  print(\"ðŸš€ Starting FastMCP server in background thread...\")\n",
        "  mcp.run(transport=\"streamable-http\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start the server in a separate thread\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Give the server a moment to start up\n",
        "time.sleep(2)\n",
        "print(\"âœ… Server should be running. Access it at http://localhost:8000/mcp\")"
      ],
      "metadata": {
        "id": "fYO0LykYQ05Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your API key\n",
        "client = OpenAI(api_key=\"your-api-key-here\")\n",
        "\n",
        "# Now you can use the OpenAI client\n",
        "# The FastMCP server is running in the background on localhost:8000\n",
        "# and can be accessed by your agent if configured to do so\n",
        "\n",
        "# Example OpenAI Agent code\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "NatMxh9xRiOK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}